# Быстрая морфология русского языка на Rust.

# Слайд 1. О себе.

Всем привет. Меня зовут Вета, я занимаюсь backend-разработкой на Rust около двух лет. Я работаю во многом с высоконагруженными системами, конкретно сейчас я поисковый инженер в компании Крибрум. Также у меня есть опыт работы с фронтендом на Rust-е.

# Cлайд 2. Крибрум.

Компания Крибрум занимается разработкой и эксплуатацией систем мониторинга и анализа открытых данных социальных медиа. Мы занимаемся как непосредственно соцсетями, так и СМИ разной величины - от "Вестника Урюпинска" до "РИА Новости" И ТАСС.
Зачастую мы оказываемся в ситуациях, когда есть потребность обрабатывать несколько гигабайт текста в секунду: повторная индексация поисковых индексов, разметка новых типов именованных сущностей, сбор данных с новых платформ и т.д. Также надо понимать, что для потоковой аналитики обработка поступающего текста должна вестись с минимальной задержкой с момента поступления.

# Слайды 3-5. Обработка естественного языка.

Специфика компании включает в себя работу с естественным языком. И для дальнейшего разговора нам понадобится несколько основных понятий.

```
Стемминг – это процесс нахождения основы слова для заданного исходного слова. Основа != корень слова. 
Лемма — словарная форма слова.
Лемматизация — процесс приведения словоформы к лемме. (также нормализация)
    сущ, прил.		    → им.п., ед.ч., м.р.
    гл., прич., деепр.  → инфинитив
Граммема – грамматическая характеристика слова. (то, что в школе было в морфологическом разборе)
    множественное число, деепричастие 
```

# Слайд 6. Обработка естественного языка. Морфологизация.

В обработке естественного языка есть два основных вектора развития - "традиционный" и с использованием нейросетей. О последнем мы сегодня говорить подробно не будем. В традиционном же подходе при работе с поточным текстом разбор слов может вестись различным образом, который можно представить как вариации от бессловарной до словарной морфологии.

Самым ярким представителем бессловарной морфологии является стеммер [Snowball](https://snowballstem.org/algorithms/russian/stemmer.html). В нем есть некоторое количество правил, по которым отсекаются окончания и слова приводятся к общей основе слова. Одна из продвинутых версий бессловарной работы, т.н. бессловарная морфология с гипотезами - при отсечении окончаний сохранять представления о частях речи.
[Статья создателя `Snowball` Мартина Портера](https://snowballstem.org/texts/introduction.html) отлично иллюстрирует работу стемминга для английского словаря, однако в русском языке мы часто сталкиваемся со словами с чередующимися корнями и другими изменениями формы, которые после работы стеммера не считаются одинаковыми: `вытер-вытирать`, `расстелить-расстилать`, `вычет-вычитание`, `был-будет`.

Словарная морфологизация разрешает чередование и изменения основы слова, а также сохраняет части речи и соответствующие им граммемы. Сразу нужно оговориться, что качество словарного морфологизатора напрямую зависит от качества словаря, взятого в основу. Составление словаря может быть затратно, поэтому часто обращаются к стеммеру как к быстрому решению.


В связи с тем, что основной фокус компании направлен на русскоязычные медиа, текста на этом языке у нас много. Самым популярным открытым решением для русского языка является [Pymorphy2](https://github.com/pymorphy2/pymorphy2/blob/92d546f042ff14601376d3646242908d5ab786c1/docs/index.rst), написанный на Python и основывающийся на словарном корпусе [OpenCorpora](https://opencorpora.org/).

С точки зрения разработки, проект какое-то время назад считается заброшенным, однако он до сих пор является частью некоторых систем (в т.ч. проекта [`Natasha`](https://github.com/natasha/natasha), также написанного на Python и включающего в себя синтаксический разбор текста, выделение именованных сущностей и многое другое, [`DeepPavlov`](https://github.com/deeppavlov/DeepPavlov), этакий NLP-комбайн для, библиотеку для построения диалоговых систем, и некоторое количество опенсорс-проектов[1]). Помимо этого, хотелось бы сказать, что грамматика языка в целом не меняется быстро, часть правил является устоявшейся, поэтому говорить об обновлении в контексте морфологизации сложно.
Разумеется, особо заинтересованным лицами поддержка проекта была продолжена в рамках [`Pymorphy3`](https://github.com/no-plagiarism/pymorphy3).

[1] Проекты:
https://github.com/rominf/profanity-filter
https://github.com/bureaucratic-labs/dostoevsky
https://github.com/SergeyShk/ruTS

# Слайды 7-8. Обработка естественного языка. Омонимия.

Какую бы морфологию мы не выбрали, мы неизбежно сталкиваемся с омонимией[1]. Однако для стеммера и словарной морфологизации проблема с омонимией будет различной.
Стеммер может применить правила там, где они не нужны (отрезать окончание глагола от существительного "нарвал", например). Словарная морфология же либо выдаст все варианты, обеспечив нас избыточностью, либо выберет наиболее вероятный вариант, который будет отобран на основе статистики по имеющимся датасетам.

[1] Омонимы — одинаковые по написанию и звучанию, но разные по значению слова и другие единицы языка.

# Слайд 9. Словари и корпусы текстов.

Имеющиеся данные для словаря - это корпус размеченных текстов или составленный по этому корпусу словарь. И здесь все будет зависеть от того, какие данные были на входе.

Вслед за `Pymorphy2` мной был взят словарь `OpenCorpora`. Разумеется, есть альтернативы, которые можно использовать в основе морфологизатора:

- UD Russian SynTagRus,
- НКРЯ (Национальный Корпус Русского Языка),
- тональный словарь русского языка КартаСловСент,
- любая компиляция вышеуказанных словарей.

`OpenCorpora` был оставлен постольку, поскольку первоначально данный проект был Proof-of-Concept под задачи компании, где необходимым требованием является производительность. Для PoF `OpenCorpora` было достаточно. В дальнейшем внутри компании планируется создать свой словарь, в т.ч. за счет компиляции имеющихся корпусов текстов и словарей.

# Слайд 10. Morphs-rs (Мопс).

Основываясь на `Pymorphy` и словаре `OpenCorpora`, мной был написан проект `Morph-rs` или Мопс, о котором сегодня и пойдет речь. Как уже говорилось выше, специфика работы включает в себя огромное количество текста в секунду, особенно текста из дикого интернета, и этот мощный поток нужно уметь быстро распознавать, обрабатывать и анализировать.

Экосистема Rust предоставляет множество решений для оптимизации производительности, о которых мы и будем говорить в процессе такого реализации `Pymorphy2` "на стероидах".

Как `Pymorphy2`, так и Мопс включают в себя предварительную работу по парсингу словаря и морфологизацию:

- парсинг слова.
- нормализация слова (лемматизация).
- приведение к нормальной форме (без изменения части речи).

# Слайд 11. Парсинг словаря OpenCorpora.

`OpenCorpora` можно [скачать из открытого доступа](https://opencorpora.org/?page=downloads), основные файлы представлены в формате `.xml` и достаточно структурированы, что можно видеть из [файла](/data/test/test_dict.xml)/

Сам словарь представлен в __структуре__ `lemmata` - слова в их начальной форме и всех спряжениях-склонениях. К каждой форме даются __граммемы__. Они могут быть не исчерпывающие, но сейчас нам придется мириться с погрешностями open source.

Слова в корпусе текстов представлены как токены и также представлены с граммемами.

# Слайд 12. Парсинг граммем.

При парсинге данных мы сразу же встречаемся с преимуществом модели памяти в `Rust`-е:

В `Pymorphy2` граммемы собираются в `frozenset`, что раздувает кучу и усложняет работу GC из-за чего растут паузы.

В `Rust` мы весь набор граммем, которые перечислены в [документации `Pymorphy2`](https://pymorphy2.readthedocs.io/en/stable/user/grammemes.html) можем представить как компактные `unit-enum`-ы, которые мы аннотируем через `serde` и парсим при помощи `quick_xml`. Сам enum представлен компактной структурой данных на стеке, каждый тип занимает один байт.

# Cлайд 13. __Важное отступление.__ Нормализация и начальная форма.

Из [документации `Pymorphy2`](https://pymorphy2.readthedocs.io/en/stable/internals/dict.html) мы знаем, что ` все связанные лексемы просто объединяются в одну большую лексему на этапе подготовки (компиляции) исходного словаря; в скомпилированном словаре информация о связях между лексемами в явном виде недоступна`. Данные связи, представленные в `OpenCorpora` как `link_types` и `links`, как раз позволяют нам следовать __лемматизации__ ("процессу приведения словоформы к лемме") или нормализации. При этом, не связывая лексемы, мы можем получить и начальную формы, не изменяя слово по части речи.

В версии 0.1.0 Мопса было принято решение не связывать леммы, в дальнейших релизах нормализация и приведение к начальной форме будет реализовано в полном объеме. Это сильно расширило количество слов в нашем словаре, но не сильно увеличило количество потребляемой памяти.

# Слайд 14. Хранение в Мопсе.

Хранение в Мопса представлено из нескольких частей.
- словарь (слово - идентификатор разбора(парсинга)),
- набор парсингов слов
- набор тегов,
- набор лемм,
- парадигмы `Vanga`.

# Слайд 15. Fst.

Для хранения словаря мной был выбран крейт `fst`. У данного крейта есть восхитительная статья [Index 1,600,000,000 Keys with Automata and Rust](https://blog.burntsushi.net/transducers/) с подробной теорией по конечному ацикличному автомату, примерами работы `fst` и прочим, с которой рекомендую ознакомиться на досуге.

Ближайшие несколько слайдов я буду обращаться к его иллюстрациям для небольшого экскурса.

# Слайд 15. FSM.

Конечный автомат (*finite state machine*, *FSM*) - это некоторое количество __состояний__ и __переходов__ между этими состояниями. Одно из этих состояний обычно помечается как начальное состояние. Ноль или более состояний могут быть помечены как конечные. 
В FSM мы всегда можем находится только в одном состоянии.
На иллюстрации мы видим конечный автомат быта кота создателя `fst`.

# Слайд 16. FSA (DAFSA).

Нам же потребуется несколько усложнить задачу, наложив несколько ограничений. Необходимо упорядочить наши данные.

Для этого нам потребуется Детерминированный ациклический приемщик ("ассептор") конечных состояний (_deterministic acyclic finite state acceptor_, *DAFSA*, *FSA*).

Он
1) детерминированный, т.е. существует не более одного перехода, который можно пройти для любого входного значения.
2) ацикличный,
3) принимающий, что означает, что конечный автомат «принимает» определенную последовательность входных данных тогда и только тогда, когда он находится в «конечном» состоянии в конце последовательности входных данных. 

Данный конечный автомат также называют __направленным ациклическим графом слов__. Он очень удобен для строк, поскольку позволяет выполнить операцию поиска строки в множестве за время, пропорциональное длине строки.

В прикладном значении, в т.ч. на уровне интерфейса, FSA воспринимается как HashSet.

# Слайд 14. FST (DAFST).

Но что если к строке нам нужно положить значение? Здесь мы и приходим к fst:  Детерминированный ациклический преобразователь конечных состояний (_deterministic acyclic finite state transducer_, *DAFST*, *FST*).

Он
1) детерминированный, т.е. существует не более одного перехода, который можно пройти для любого входного значения.
2) ацикличный,
3) преобразовывающий, что означает, что конечный автомат выдает значение, связанное с определенной последовательностью входных данных, подаваемых в машину. Значение выдается тогда и только тогда, когда последовательность входных данных приводит к завершению работы машины в конечном состоянии.

При этом, как представлено в конце статьи, именно на огромных данных fst начинает превосходить как сжатие, так и поиск по этим данным. Естественно, для большого словаря `fst` как нельзя кстати мог бы нам пригодится. В одной только `OpenCorpora` заявлено [391842 лемм](https://opencorpora.org/dict.php), если также включить все формы (во всех склонениях, спряжениях), то данная цифра только возрастает.

# Слайд 18. Парадигмы Pymorphy.

Зачем же нам нужен FST? Здесь мы должны обратиться к тому, как `Pymorphy2` хранит внутри себя слова. 

Каждое слово имеет разные формы по склонению/спряжению. У форм есть свои приставки, окончания и наборы граммем (теги). В `Pymorphy2`, вспомогательно используется стемминг, о котором мы говорили в начале, но стемма создается не по набору правил отсечения, а через нахождение наибольшей основы слова в связке `lemma` из словаря `OpenCorpora`.

Например, для слова __хомяковый__:

```
ПРЕФИКС           ХВОСТ   ТЕГ
                  ый      ADJF,Qual masc,sing,nomn
                  ого     ADJF,Qual masc,sing,gent
                  ...
                  ы       ADJS,Qual plur
                  ее      COMP,Qual
                  ей      COMP,Qual V-ej
    по            ее      COMP,Qual Cmp2
    по            ей      COMP,Qual Cmp2,V-ej
```

Полученные таблицы в `Pymorphy2` называются парадигмами. Для более компактного хранения `Pymorphy2` преобразует парадигмы в массивы чисел. [Префиксам, “хвостам” и тегам (наборам граммем) присваиваются номера, и в *парадигмах* хранятся только эти номера. Строки с префиксами, хвостами и тегами хранятся отдельно, в питоновых list. Номер строки - это просто ее индекс.](https://pymorphy2.readthedocs.io/en/stable/internals/dict.html#id4). 

# Слайд 19. Хранение парадигм Pymorphy.

`<слово> <разделитель> <номер парадигмы> <номер формы в парадигме>` упаковываются в [DAWG, т.е. Deterministic Acyclic Finite State Automaton](https://en.wikipedia.org/wiki/Deterministic_acyclic_finite_state_automaton). 

У `Pymorphy2` используются две библиотеки [DAWG](https://github.com/kmike/DAWG) (это обертка над C++ библиотекой [dawgdic](https://code.google.com/p/dawgdic/)) или [DAWG-Python](https://github.com/kmike/DAWG-Python) (это написанная на питоне реализация DAWG, которая не требует компилятора для установки и работает быстрее [DAWG](https://github.com/kmike/DAWG) под PyPy). 

--

Можно заметить, что в этом случае нам приходится при вычленении строки из конечного автомата дополнительно распарсить разделитель, номер парадигмы и номер формы в парадигме, что занимает время и ресурсы.

Fst же сразу выдает нам идентификатор из набора разборов (парсингов) слов.

# Слайд 20. Хранение в Мопсе. Словарь.

К каждому слову и его форме у нас есть свой набор "разборов". Для одинаковых слов разной грамматики (например, "стали" от сталь или стать) разборов будет несколько и у этого __набора разборов__ будет свой числовой идентификатор. Числовым идентификатором, в данном случае, выступает индекс набора разборов в векторе.

Для разбора нам нужны три составляющие:
- форма: нормализованная, (в буд. еще начальная) и "другая" 
- индекс из набора уникальных тегов
- индекс из набора уникальных лемм (в будущем их количество будет сильно меньше, чем кол-во лемм)

``` rust 
let map = fst::Map<Vec<u8>>; // <word, id в ParseTable>

type ParseTable = Vec<Vec<Parse>>;

struct Parse {
    form: Form,
    tag: TagID,           // id в Tags
    normal_form: LemmaId, // id в Lemmas
}

enum Form {
  Normalize,
  Different,
}

pub type Tags = Vec<Tag>;
pub type Lemmas = Vec<SmallString<[u8; 16]>>;

pub type Tag = SmallVec<[Grammem; 8]>;
```

Тег, по наследию от `Pymorphy` - это *один* набор граммем. В среднем у слова около 8 граммем, поэтому, чтобы избежать множественных аллокаций "с запасом" в куче, мы взяли `SmallVec`, который аллоцирует данные на стеке. Для экстра длинных данных у нас все же будет аллокация в куче, но это сильно меньше, чем если бы мы хранили там все данные (Rust всегда выделяет место в куче с сильным запасом).

Так как нам приходится сохранять лемму, нам нужно компактное хранение строк. Здесь мы взяли `SmallString`, который можно сложить на стек. В случае, если у нас окажется слово большее, чем выделенное количество байт, будет аллокация на куче, все аналогично `SmallVec`.

Почему мы берем так много векторов? Все достаточно просто: `Rust` последовательно записывает значения в плоский сегмент памяти, сохраняя их порядок и позволяя нам обращаться к индексу как "ключу", не храня отдельно номер ключа в памяти. Обращение по ключу условно бесплатное.
Еще одна оптимизация ;)

С векторами же нам удобно работать и на моменте компиляции морфологического анализатора - бинарный поиск позволяет быстро вычислить соответствующий ID (и для тегов, и для лемм). 

# Слайд 21. Pymorphy2. Score.

Должна заметить, что в `Pymorphy2` также наличествуют `Score` при парсинге слова, которых нет у Мопса. `Score` достаются из размеченного корпуса текстов. В каждом предложении корпуса все слова разбиты по соответствующим их положению граммемам. Собрав все такие упоминания для каждого слова во всех его значениях и получается частота упоминания.
Например, для того же слова "стали" форму глагола мы встречаем 9000 раз, а форму существительного в родительном падеже около 1000 раз. Соответственно, вероятнее всего при парсинге отдельного слова "стали" - это глагол.
В Мопсе таких `Score` нет по практическим нуждам, о которых я расскажу в конце.

# Слайд 22. Сборка.

Один из нюансов работы с `fst`: необходимость выстроить слова в алфавитном порядке. Это требует от нас на этапе сборки слов не просто собрать их, но с помощью `BTreeMap` предварительно отсортировать их.

`let mut word_map: BTreeMap<String, Vec<Parse>> = BTreeMap::new();`

Помимо этого мы знаем, что нам нужно собрать наборы `Parse` к каждому слову. Для каких-то слов это будет один набор (`vec.len() = 1`), а для каких-то несколько наборов. Узнать мы это можем только непосредственно проходя по всем вычлененным словам, меняя вектор в `Value` нашей `HashMap`. Здесь нам, конечно же, куда удобнее воспользоваться `Entry`, который проходит по всей мапе и изменяет найденное значение, соответствующее условиям. Это сокращает в два раза время компиляции для слов по сравнению с `IterMut`. 

В итоге сборка Мопса (на машине с AMD Ryzen™ 9 7900X × 24 и 64 Гб оперативной памяти) составляет всего: 

- time:   [29.340 s **29.613 s** 29.867 s]
- thrpt:  [13.435 MiB/s **13.550 MiB/s** 13.676 MiB/s]


# Слайд 23. Ванга.

Есть в словарной морфологии еще одна проблема - словарь конечный, а язык (как набор слов и выражений) - нет. Несмотря на то, что мы используем преимущественно словарную морфологизацию, для предугадывания незнакомых словарю слов нам понадобятся парадигмы `Pymorphy2` (но немного по-другому).

NB: Сейчас в полной объеме предугадывание слов не реализовано, но сборка `Vanga`s ведется.

# Слайд 24. Ванга.

`Pymorphy2` собирает для предугадывания не всю парадигму, а только ее "хвост", постфикс. У постфиксов наборы тегов могут отличаться от тех, что есть у слов. Поэтому для каждого набора префикса-постфикса (окончания) сохраняется свой набор тегов. Наборы постфикс-форма-теги составляют `Vanga`, структуру для предугадывания. Как было замечено еще в `Pymorphy2`, таких наборов будет сильно меньше, чем слов, что позволяет сильно сократить хранение в памяти. Помимо этого, для `Vanga` есть еще несколько правил, о которым мы поговорим ниже, сокращающих количество Ванг.

``` rust
/// Уникальный набор постфикса, формы и тега к нему.
#[derive(Debug, Clone, PartialEq, Eq, Hash, PartialOrd, Serialize, Deserialize)]
pub struct VangaItem {
    postfix: smallstr::SmallString<[u8; 8]>,
    form:  Form,
    tag: TagID, // индекс в Tags
}

#[derive(Debug, Clone, PartialEq, Eq, Hash, PartialOrd, Serialize, Deserialize)]
/// `Vanga` - предсказание на основе постфикса.
/// `popularity` - частотность встречаемой `Vanga`, позволяющей как отсортировать парсинг по вероятности, так и убрать малопопулярные постфиксы.
pub struct Vanga {
    popularity: u64,
    items: Vec<VangaItem>,
}
```

# Слайд 25. Vanga. Ограничения.

Вслед за `Pymorphy2` у нас есть еще несколько ограничений, сильно сокращающих количество `Vanga`:

- не все части речи могут иметь суффиксы-окончания. Например, предлоги, междометия. Для таких частей речи мы не храним `Vanga`,
- максимальная длина окончания не должна быть больше 5,
- редкие окончания, т.е. встреченные не более одного раза, удаляются,
- разборы непродуктивных парадигм, т.е. менее 3 лексем в словаре, удаляются.

# Слайд 26. Парсинг и нормализация слова Мопсом.

Итак, морфологизатор собран. Теперь осталось им воспользоваться. 

Из `Pymorphy2` мной взяты основные функции, которыми может воспользоваться потенциальный пользователь:

- парсинг, возвращающий слово, наборы тегов и их нормальные формы,
- просмотр тегов распознанного, проверка наличия тега у распознанного, выбор парсинга по наличию в нем тега,
- нормализация и приведение к начальной форме,
- склонение слова,
- согласование с числительными.

На данный момент, две последние функции не реализованы в Мопсе, однако они спокойно исходят из имеющихся структур и предыдущих функций.

Нормализация и приведение к начальной форме также в настоящий момент в Мопсе является одной и той же функцией, как уже говорили ранее, и будут реализованы в дальнейшем.

Парсинг и нормализация в Мопсе в релизе 0.1.0 работают только со словарными словами.

Мы смотрим, есть ли слово в словаре. Если оно наличествует, то мы берем идентификатор набора разборов и получаем все разборы слова с нормальными формами к каждому разбору.

# Cлайд 27. Парсинг и нормализация. Бенчмарки.

Pymorphy в процессе обработки слов раздувается в оперативной памяти аж до 8 гб. Логичным выходом является воспользоваться GC, однако просадка от этого становится просто ужасной: 

time:  
    [8.5910 s **10.443 s** 12.329 s]
thrpt:
    [49.464 KiB/s **58.399 KiB/s** 70.986 KiB/s]

В то же время парсинг и нормализация Мопса на машине с AMD Ryzen™ 9 7900X × 24 и 64 Гб оперативной памяти:

Parse:
- time:   [16.091 ms **16.174 ms** 16.276 ms]
- thrpt:  [38.792 MiB/s **39.038 MiB/s** 39.238 MiB/s]

Normalize:
- thrpt:  [28.646 ms **28.842 ms** 29.068 ms]
- thrpt:  [21.721 MiB/s **21.891 MiB/s** 22.041 MiB/s]


# Слайд 28-29. Зачем Мопс нужен?

На данный момент Мопс уже тестируется в нашей компании для более точного поиска по документам. В огромном потоке информации всегда хочется найти что-то более конкретное. Однако при работе со стеммером на одно слово можно найти множество вариаций.

Из совсем простых примеров:
`пожар -> пожарить, пожарный (человек), пожар, пожарный (прилагательное)`

Уточнение через поисковый запрос части речи отсеивает нам не те примеры.

# Слайд 30. Зачем Мопс нужен?

Возвращаясь к тому же, с чего мы начали. Чередующиеся корни и омонимия. Мы можем видеть пример работы `Snowball`-а и того же `Pymorphy2`, где второй дает нам корректную нормализацию и соотносит слова в разных формах к одной лемме.

# Слайд 31. Зачем Мопс нужен? BERT.

Заглядывая совсем в будущее, можно смело шагнуть и к теме нейросетей. `BERT` — это архитектурная модель для нейросетей. Говорить подробнее о ней я не буду, так как это лишь далеко прогнозируемые планы, а устройство лучше изложено в многочисленных статьях, например на том же `habr.com`.[1]

Что нам важно, так это возможность передать тексты BERT-модели и из нее получать, например, наиболее вероятные теги для слов (части речи, число и т.д.). Передав слово с тегом с наибольшей вероятностью Мопсу, мы получим нормализованную форму. Поиск по нормализованной форме позволяет нам вернуть все склонения.

Говоря о предыдущем слайде, вводя "хотеть" мы __хотим :)__ видеть документы и с "хочу", и с "хотел".

[1] https://habr.com/ru/companies/otus/articles/702838/

# Слайд 32. Что дальше? 

Разумеется, имеющийся морфологизатор не является законченным ни с точки зрения воплощения всех функций `Pymorphy2`, ни с точки зрения стоящих передо мной задач. По ходу доклада мы уже встречались с такими неразрешенностями, как 

- приведение при нормализации слова в другую часть речи (деепричастие/причастие к глаголу),
- возможность подключения других словарей, совмещения нескольких словарей,
- предугадывание слова по имеющимся `Vanga`-м,
- сочетание с числительными.

Помимо этого, Мопс будет интегрироваться с нейросетями.

# Слайд 33. Где мы кайфанули?

Ну и напоследок, за что мы ценим Rust - где нам удалось кайфануть на оптимизациях?

- автоматизированный парсинг через quick_xml + serde,
- упрощенная модель хранения,
- Entry для HashMap и BtreeMap,
- оптимизация хранения (SmallString, SmallVec),
- убираем лишний парсинг для value из FSA → FST.

Список затрагивает, конечно, самые крупные части оптимизаций. Все мелкие реализации можно будет увидеть в коде.

# Слайд 34. Спасибо за внимание.